{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "584984a6",
   "metadata": {},
   "source": [
    "### Next Tasks\n",
    "* class weight\n",
    "* add a new dense trainable layer after the huggingface model (Done)\n",
    "* add a final sigmoid dense layer to get the probabilities of the classes (Done)\n",
    "* change metrics to auc (Done)\n",
    "* freeze pretrained model layers, add a new dense layer and train the model only for this new layer ( transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30b5ede",
   "metadata": {},
   "source": [
    "## Classification Problem: Spam Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99f36e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import pipeline, DistilBertTokenizerFast, TFDistilBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e306a758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['activation_13', 'vocab_transform', 'vocab_projector', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier', 'classifier', 'dropout_19']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcc6e9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2cd75db",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = pd.read_csv('spam.csv')[['v1','v2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dbde58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata['target'] = np.where(mydata['v1']=='ham',0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e971a034",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata.drop(columns=['v1'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d778dfd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v2</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  v2  target\n",
       "0  Go until jurong point, crazy.. Available only ...       0\n",
       "1                      Ok lar... Joking wif u oni...       0\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...       1\n",
       "3  U dun say so early hor... U c already then say...       0\n",
       "4  Nah I don't think he goes to usf, he lives aro...       0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2459897b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX,testX, trainY, testY = train_test_split(mydata['v2'],mydata['target'],stratify=mydata['target'],test_size=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db25561e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX.reset_index(inplace=True,drop=True)\n",
    "testX.reset_index(inplace=True,drop=True)\n",
    "trainY.reset_index(inplace=True,drop=True)\n",
    "testY.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d03c1d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_max_length=221"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a91c92d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX_encoded = tokenizer(trainX.to_list(),padding='max_length',truncation=True,max_length=my_max_length)\n",
    "testX_encoded = tokenizer(testX.to_list(),padding='max_length',truncation=True,max_length=my_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a1e0de87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=221, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX_encoded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ffec449e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=221, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testX_encoded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "29a4c235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY[trainY==1].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7aec18b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 1061,\n",
       " 24654,\n",
       " 3013,\n",
       " 2205,\n",
       " 2460,\n",
       " 3393,\n",
       " 2232,\n",
       " 1012,\n",
       " 1057,\n",
       " 24654,\n",
       " 2066,\n",
       " 6289,\n",
       " 1029,\n",
       " 2016,\n",
       " 3478,\n",
       " 1012,\n",
       " 2016,\n",
       " 1005,\n",
       " 1055,\n",
       " 3243,\n",
       " 6517,\n",
       " 1012,\n",
       " 102,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX_encoded['input_ids'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b08c64ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(trainX_encoded),trainY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7045c330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'input_ids': array([  101,  1061, 24654,  3013,  2205,  2460,  3393,  2232,  1012,\n",
       "           1057, 24654,  2066,  6289,  1029,  2016,  3478,  1012,  2016,\n",
       "           1005,  1055,  3243,  6517,  1012,   102,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0], dtype=int32)},\n",
       "  0)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## as_numpy_iterator = > Returns an iterator which converts all elements of the dataset to numpy.\n",
    "\n",
    "list(train_dataset.as_numpy_iterator())[10:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3b57f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(testX_encoded),testY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84878477",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = tf.data.Dataset.from_tensor_slices((dict(testX_encoded)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c2f12c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for element in train_dataset.shuffle(1000).batch(10):\n",
    "    count = count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44066523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "390"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a985598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3900,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fb2c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9b13200a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        [(None, 221)]             0         \n",
      "_________________________________________________________________\n",
      "tf_distil_bert_for_sequence_ TFSequenceClassifierOutpu 66955010  \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 256)               768       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 66,956,035\n",
      "Trainable params: 66,956,035\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_layer = tf.keras.layers.Input(shape = (221,), dtype='int64')\n",
    "distbert = model(input_layer)\n",
    "distbert = distbert[0]              \n",
    "flat = tf.keras.layers.Flatten()(distbert)\n",
    "dense = tf.keras.layers.Dense(units=256, activation=tf.keras.activations.relu)(flat) # Adding Additional Linear Layer\n",
    "classifier = tf.keras.layers.Dense(units=1, activation=tf.keras.activations.sigmoid)(dense)\n",
    "mymodel = tf.keras.Model(inputs=input_layer, outputs=classifier)\n",
    "mymodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b86ac143",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "mymodel.compile(optimizer=optimizer, \n",
    "              loss=tf.keras.losses.binary_crossentropy,\n",
    "              metrics = tf.keras.metrics.AUC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "272e6824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 1336s 22s/step - loss: 0.3962 - auc_11: 0.9629\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd281b3b3d0>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymodel.fit(np.array(trainX_encoded['input_ids']),np.array(trainY), epochs=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "702db115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3900,)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(trainY).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "6762d885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3900, 221)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(trainX_encoded['input_ids']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "0e31d611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x7fd2c07bea90>,\n",
       " <transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertForSequenceClassification at 0x7fd5ae628190>,\n",
       " <keras.layers.core.Flatten at 0x7fd2c07beb50>,\n",
       " <keras.layers.core.Dense at 0x7fd2c05a6c90>,\n",
       " <keras.layers.core.Dense at 0x7fd2a9a06b50>]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymodel.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea304ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "38ab41a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_proba = mymodel.predict(np.array(testX_encoded['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3153d93",
   "metadata": {},
   "source": [
    "* The closer pred_proba is to 0.0 the more likely it is class 0 and when it is closer to 1.0 then it is more likely that it is class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b0d28fad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1380687 ],\n",
       "       [0.13807476],\n",
       "       [0.13807559],\n",
       "       [0.8192456 ],\n",
       "       [0.13805553]], dtype=float32)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_proba[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "9f8e3aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    1\n",
       "4    0\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "8df72f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "760a1b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9955927510852407"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(testY,pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab4eb72",
   "metadata": {},
   "source": [
    "* Additonal Dense Layer Model has beaten the previous core model (part-1)\n",
    "* 0.9899 vs 0.9955"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc6e2c0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c03280e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
