{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c30b5ede",
   "metadata": {},
   "source": [
    "## Classification Problem: Spam Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99f36e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import pipeline, DistilBertTokenizerFast, TFDistilBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e306a758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_layer_norm', 'activation_13', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier', 'classifier', 'dropout_19']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcc6e9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2cd75db",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = pd.read_csv('spam.csv')[['v1','v2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dbde58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata['target'] = np.where(mydata['v1']=='ham',0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e971a034",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata.drop(columns=['v1'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d778dfd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v2</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  v2  target\n",
       "0  Go until jurong point, crazy.. Available only ...       0\n",
       "1                      Ok lar... Joking wif u oni...       0\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...       1\n",
       "3  U dun say so early hor... U c already then say...       0\n",
       "4  Nah I don't think he goes to usf, he lives aro...       0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2459897b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX,testX, trainY, testY = train_test_split(mydata['v2'],mydata['target'],stratify=mydata['target'],test_size=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db25561e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX.reset_index(inplace=True,drop=True)\n",
    "testX.reset_index(inplace=True,drop=True)\n",
    "trainY.reset_index(inplace=True,drop=True)\n",
    "testY.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a91c92d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX_encoded = tokenizer(trainX.to_list(),padding=True,truncation=True)\n",
    "testX_encoded = tokenizer(testX.to_list(),padding=True,truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1e0de87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=186, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX_encoded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffec449e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29a4c235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7    1\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY[trainY==1].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7aec18b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 1045,\n",
       " 14145,\n",
       " 2080,\n",
       " 26947,\n",
       " 1012,\n",
       " 1012,\n",
       " 1012,\n",
       " 2066,\n",
       " 24654,\n",
       " 5292,\n",
       " 2546,\n",
       " 1012,\n",
       " 1012,\n",
       " 1012,\n",
       " 102,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX_encoded['input_ids'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b08c64ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(trainX_encoded),trainY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7045c330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'input_ids': array([  101,  1045, 14145,  2080, 26947,  1012,  1012,  1012,  2066,\n",
       "          24654,  5292,  2546,  1012,  1012,  1012,   102,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)},\n",
       "  0)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## as_numpy_iterator = > Returns an iterator which converts all elements of the dataset to numpy.\n",
    "\n",
    "list(train_dataset.as_numpy_iterator())[10:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3b57f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(testX_encoded),testY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84878477",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = tf.data.Dataset.from_tensor_slices((dict(testX_encoded)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c2f12c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for element in train_dataset.shuffle(1000).batch(10):\n",
    "    count = count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44066523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "390"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a985598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3900,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86ac143",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "model.compile(optimizer=optimizer, \n",
    "              loss=model.compute_loss) # can also use any keras loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60608cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 1553s 26s/step - loss: 0.1042\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff7294be590>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## When you use batch with the dataset, you don't need to define the batch_size in the fit method\n",
    "## source: https://stackoverflow.com/questions/62670041/batch-size-in-tf-model-fit-vs-batch-size-in-tf-data-dataset\n",
    "\n",
    "model.fit(train_dataset.shuffle(1000).batch(64), epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0e31d611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertMainLayer at 0x7ff738aafe50>,\n",
       " <keras.layers.core.Dense at 0x7ff738f3cf90>,\n",
       " <keras.layers.core.Dense at 0x7ff738f642d0>,\n",
       " <keras.layers.core.Dropout at 0x7ff738f64690>]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "38ab41a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw_pred = model.predict(testX_encoded['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e48a53f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_proba = tf.math.softmax(raw_pred[0], axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "873da8c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.79919356,\n",
       " 0.6529633,\n",
       " 0.6586566,\n",
       " 0.87410295,\n",
       " 0.6491943,\n",
       " 0.65674955,\n",
       " 0.64292157,\n",
       " 0.6555667,\n",
       " 0.6694563,\n",
       " 0.8525124]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pred_proba[0:10][:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "39bb85e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    0\n",
       "2    0\n",
       "3    1\n",
       "4    0\n",
       "5    0\n",
       "6    1\n",
       "7    0\n",
       "8    0\n",
       "9    1\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8df72f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "760a1b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9899121941594318"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(testY,pred_proba[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657a28ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6cc6e2c0",
   "metadata": {},
   "source": [
    "### Next Tasks\n",
    "* class weight\n",
    "* add final layer to get the probabilities\n",
    "* change metrics to auc\n",
    "* freeze few layers and train the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c03280e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
